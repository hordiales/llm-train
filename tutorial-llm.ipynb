{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Tutorial: https://www.datacamp.com/tutorial/llama3-fine-tuning-locally","metadata":{}},{"cell_type":"markdown","source":"# Fine-Tuning Llama 3\nFor this tutorial, weâ€™ll fine-tune the Llama 3 8B-Chat model using the ruslanmv/ai-medical-chatbot dataset. The dataset contains 250k dialogues between a patient and a doctor. Weâ€™ll use the Kaggle Notebook to access this model and free GPUs.\n\nSetting up\nBefore we launch the Kaggle Notebook, fill out the Meta download form with your Kaggle email address, then go to the Llama 3 model page on Kaggle and accept the agreement. The approval process may take one to two days.\n\nLetâ€™s now take the following steps:\n\n# Launch the new Notebook on Kaggle, and add the Llama 3 model by clicking the + Add Input button, selecting the Models option, and clicking on the plus + button beside the Llama 3 model. After that, select the right framework, variation, and version, and add the model.\n\nAdding LLama 3 model into the Kaggle notebook\n\n# Go to the Session options and select the GPU P100 as an accelerator.\n\nChanging the accelerator to GPU P100 in Kaggle\n\n# Generate the Hugging Face and Weights & Biases token, and create the Kaggle Secrets. You can create and activate the Kaggle Secrets by going to Add-ons > Secrets > Add a new secret.\n\nSetting up secrets (environment variables)\n\n# Initiate the Kaggle session by installing all the necessary Python packages.","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:21:59.269203Z","iopub.execute_input":"2024-06-11T20:21:59.269900Z","iopub.status.idle":"2024-06-11T20:23:25.599186Z","shell.execute_reply.started":"2024-06-11T20:21:59.269856Z","shell.execute_reply":"2024-06-11T20:23:25.597918Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install -U accelerate ","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:23:25.600535Z","iopub.execute_input":"2024-06-11T20:23:25.600808Z","iopub.status.idle":"2024-06-11T20:23:37.998220Z","shell.execute_reply.started":"2024-06-11T20:23:25.600783Z","shell.execute_reply":"2024-06-11T20:23:37.997189Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.31.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import the necessary Python pages for loading the dataset, model, and tokenizer and fine-tuning.","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:23:37.999541Z","iopub.execute_input":"2024-06-11T20:23:37.999834Z","iopub.status.idle":"2024-06-11T20:23:46.484290Z","shell.execute_reply.started":"2024-06-11T20:23:37.999806Z","shell.execute_reply":"2024-06-11T20:23:46.483331Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-06-11 20:23:41.312255: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-11 20:23:41.312313: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-11 20:23:41.313870: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":" Weâ€™ll be tracking the training process using the Weights & Biases and then saving the fine-tuned model on Hugging Face, and for that, we have to log in to both Hugging Face Hub and Weights & Biases using the API key.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on Medical Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:23:46.487558Z","iopub.execute_input":"2024-06-11T20:23:46.488175Z","iopub.status.idle":"2024-06-11T20:24:05.634836Z","shell.execute_reply.started":"2024-06-11T20:23:46.488144Z","shell.execute_reply":"2024-06-11T20:24:05.633505Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhordiales\u001b[0m (\u001b[33mullm\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240611_202348-se1aip5r</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ullm/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/se1aip5r' target=\"_blank\">atomic-gorge-11</a></strong> to <a href='https://wandb.ai/ullm/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ullm/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/ullm/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ullm/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/se1aip5r' target=\"_blank\">https://wandb.ai/ullm/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/se1aip5r</a>"},"metadata":{}}]},{"cell_type":"markdown","source":"Set the base model, dataset, and new model variable. Weâ€™ll load the base model from Kaggle and the dataset from the HugginFace Hub and then save the new model.","metadata":{}},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\ndataset_name = \"ruslanmv/ai-medical-chatbot\"\nnew_model = \"llama-3-8b-chat-doctor\"","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:24:05.637613Z","iopub.execute_input":"2024-06-11T20:24:05.638453Z","iopub.status.idle":"2024-06-11T20:24:05.644133Z","shell.execute_reply.started":"2024-06-11T20:24:05.638424Z","shell.execute_reply":"2024-06-11T20:24:05.643078Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Set the data type and attention implementation.","metadata":{}},{"cell_type":"code","source":"torch_dtype = torch.float16\nattn_implementation = \"eager\"","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:24:05.645688Z","iopub.execute_input":"2024-06-11T20:24:05.646019Z","iopub.status.idle":"2024-06-11T20:24:05.655356Z","shell.execute_reply.started":"2024-06-11T20:24:05.645986Z","shell.execute_reply":"2024-06-11T20:24:05.654413Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Loading the model and tokenizer\nIn this part, weâ€™ll load the model from Kaggle. However, due to memory constraints, weâ€™re unable to load the full model. Therefore, weâ€™re loading the model using 4-bit precision.\n\nOur goal in this project is to reduce memory usage and speed up the fine-tuning process.","metadata":{}},{"cell_type":"markdown","source":"Q-LoRA (Quantized Low-Rank Adaptation)","metadata":{}},{"cell_type":"code","source":"pip install -i https://pypi.org/simple/ bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:24:05.656762Z","iopub.execute_input":"2024-06-11T20:24:05.657170Z","iopub.status.idle":"2024-06-11T20:24:17.922004Z","shell.execute_reply.started":"2024-06-11T20:24:05.657140Z","shell.execute_reply":"2024-06-11T20:24:17.920802Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple/\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install -f accelerate","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:24:17.923732Z","iopub.execute_input":"2024-06-11T20:24:17.924146Z","iopub.status.idle":"2024-06-11T20:24:19.691120Z","shell.execute_reply.started":"2024-06-11T20:24:17.924102Z","shell.execute_reply":"2024-06-11T20:24:19.690001Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: You must give at least one requirement to install (maybe you meant \"pip install accelerate\"?)\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:24:19.692995Z","iopub.execute_input":"2024-06-11T20:24:19.693374Z","iopub.status.idle":"2024-06-11T20:26:03.108061Z","shell.execute_reply.started":"2024-06-11T20:24:19.693337Z","shell.execute_reply":"2024-06-11T20:26:03.106999Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d99aab5b688f40efa7032a685b05dce2"}},"metadata":{}}]},{"cell_type":"markdown","source":"Load the tokenizer and then set up a model and tokenizer for conversational AI tasks. By default, it uses the chatml template from OpenAI, which will convert the input text into a chat-like format.","metadata":{}},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:26:03.109508Z","iopub.execute_input":"2024-06-11T20:26:03.109909Z","iopub.status.idle":"2024-06-11T20:26:03.626293Z","shell.execute_reply.started":"2024-06-11T20:26:03.109870Z","shell.execute_reply":"2024-06-11T20:26:03.625000Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Adding the adapter to the layer\nFine-tuning the full model will take a lot of time, so to improve the training time, weâ€™ll attach the adapter layer with a few parameters, making the entire process faster and more memory-efficient.","metadata":{}},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:26:03.627986Z","iopub.execute_input":"2024-06-11T20:26:03.628455Z","iopub.status.idle":"2024-06-11T20:26:04.452710Z","shell.execute_reply.started":"2024-06-11T20:26:03.628412Z","shell.execute_reply":"2024-06-11T20:26:04.451507Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Loading the dataset\nTo load and pre-process our dataset, we:\n\n1. Load the ruslanmv/ai-medical-chatbot dataset, shuffle it, and select only the top 1000 rows. This will significantly reduce the training time.\n\n2. Format the chat template to make it conversational. Combine the patient questions and doctor responses into a \"text\" column.\n\n3. Display a sample from the text column (the â€œtextâ€ column has a chat-like format with special tokens).","metadata":{}},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"all\")\ndataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n\ndef format_chat_template(row):\n    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc=4,\n)\n\ndataset['text'][3]","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:26:04.453947Z","iopub.execute_input":"2024-06-11T20:26:04.454323Z","iopub.status.idle":"2024-06-11T20:26:09.361011Z","shell.execute_reply.started":"2024-06-11T20:26:04.454286Z","shell.execute_reply":"2024-06-11T20:26:09.359288Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/863 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bd5f6b6f0a54de1bd950534f6e4bb04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/142M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34e2257175e547fd86486dd40e8f9a7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/256916 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44445d9bef014b62bcc4d28a337bf9c7"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0e9c076577945e2859b75c031c0ad30"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'<|im_start|>user\\nFell on sidewalk face first about 8 hrs ago. Swollen, cut lip bruised and cut knee, and hurt pride initially. Now have muscle and shoulder pain, stiff jaw(think this is from the really swollen lip),pain in wrist, and headache. I assume this is all normal but are there specific things I should look for or will I just be in pain for a while given the hard fall?<|im_end|>\\n<|im_start|>assistant\\nHello and welcome to HCM,The injuries caused on various body parts have to be managed.The cut and swollen lip has to be managed by sterile dressing.The body pains, pain on injured site and jaw pain should be managed by pain killer and muscle relaxant.I suggest you to consult your primary healthcare provider for clinical assessment.In case there is evidence of infection in any of the injured sites, a course of antibiotics may have to be started to control the infection.Thanks and take careDr Shailja P Wahal<|im_end|>\\n'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Split the dataset into a training and validation set.","metadata":{}},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:26:09.365594Z","iopub.execute_input":"2024-06-11T20:26:09.365907Z","iopub.status.idle":"2024-06-11T20:26:09.381402Z","shell.execute_reply.started":"2024-06-11T20:26:09.365878Z","shell.execute_reply":"2024-06-11T20:26:09.380648Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Complaining and training the model\nWe are setting the model hyperparameters so that we can run it on the Kaggle. You can learn about each hyperparameter by reading the Fine-Tuning Llama 2 tutorial.\n\nWe are fine-tuning the model for one epoch and logging the metrics using the Weights and Biases.","metadata":{}},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    evaluation_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:26:09.382432Z","iopub.execute_input":"2024-06-11T20:26:09.382706Z","iopub.status.idle":"2024-06-11T20:26:09.667029Z","shell.execute_reply.started":"2024-06-11T20:26:09.382683Z","shell.execute_reply":"2024-06-11T20:26:09.665925Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Weâ€™ll now set up a supervised fine-tuning (SFT) trainer and provide a train and evaluation dataset, LoRA configuration, training argument, tokenizer, and model. Weâ€™re keeping the max_seq_length to 512 to avoid exceeding GPU memory during training.\n","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:26:09.668576Z","iopub.execute_input":"2024-06-11T20:26:09.668978Z","iopub.status.idle":"2024-06-11T20:26:10.791567Z","shell.execute_reply.started":"2024-06-11T20:26:09.668942Z","shell.execute_reply":"2024-06-11T20:26:10.790446Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ğŸ¤— Transformers. Use `--hub_token` instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc96aa49d2034f77a6a70250674ce0c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6845c650f2fe429db4c2cd7554860dbd"}},"metadata":{}}]},{"cell_type":"markdown","source":"Weâ€™ll start the fine-tuning process by running the following code.","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:26:10.792757Z","iopub.execute_input":"2024-06-11T20:26:10.793067Z","iopub.status.idle":"2024-06-11T20:55:56.185879Z","shell.execute_reply.started":"2024-06-11T20:26:10.793042Z","shell.execute_reply":"2024-06-11T20:55:56.184628Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [450/450 29:38, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>90</td>\n      <td>2.352900</td>\n      <td>2.694772</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.376900</td>\n      <td>2.594583</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>2.660900</td>\n      <td>2.537729</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>2.229600</td>\n      <td>2.499141</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.197000</td>\n      <td>2.479675</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=450, training_loss=2.7426673891809252, metrics={'train_runtime': 1784.6138, 'train_samples_per_second': 0.504, 'train_steps_per_second': 0.252, 'total_flos': 9229859418095616.0, 'train_loss': 2.7426673891809252, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"Both training and validation losses have decreased. Consider training the model for three epochs on the full dataset for better results.","metadata":{}},{"cell_type":"markdown","source":"# Model evaluation\nWhen you finish the Weights & Biases session, itâ€™ll generate the run history and summary.\n","metadata":{}},{"cell_type":"code","source":"wandb.finish()\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:55:56.187425Z","iopub.execute_input":"2024-06-11T20:55:56.188146Z","iopub.status.idle":"2024-06-11T20:56:01.164035Z","shell.execute_reply.started":"2024-06-11T20:55:56.188109Z","shell.execute_reply":"2024-06-11T20:56:01.163283Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>â–ˆâ–…â–ƒâ–‚â–</td></tr><tr><td>eval/runtime</td><td>â–â–„â–‡â–‡â–ˆ</td></tr><tr><td>eval/samples_per_second</td><td>â–â–â–â–â–</td></tr><tr><td>eval/steps_per_second</td><td>â–â–â–â–â–</td></tr><tr><td>train/epoch</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–‚â–‚â–â–â–‚â–â–â–â–‚â–‚â–‡â–â–â–‚â–â–â–‚â–â–â–ˆâ–â–â–â–â–â–‚â–â–â–‚â–â–â–â–</td></tr><tr><td>train/learning_rate</td><td>â–„â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–†â–…â–…â–…â–…â–…â–…â–„â–„â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–â–â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–…â–„â–ƒâ–ƒâ–â–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–‚â–‚â–‚â–â–‚â–‚â–‚â–‚â–‚â–‚â–â–ƒâ–‚â–ƒâ–â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–ƒâ–‚</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.47968</td></tr><tr><td>eval/runtime</td><td>77.58</td></tr><tr><td>eval/samples_per_second</td><td>1.289</td></tr><tr><td>eval/steps_per_second</td><td>1.289</td></tr><tr><td>total_flos</td><td>9229859418095616.0</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>450</td></tr><tr><td>train/grad_norm</td><td>1.96832</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.197</td></tr><tr><td>train_loss</td><td>2.74267</td></tr><tr><td>train_runtime</td><td>1784.6138</td></tr><tr><td>train_samples_per_second</td><td>0.504</td></tr><tr><td>train_steps_per_second</td><td>0.252</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">atomic-gorge-11</strong> at: <a href='https://wandb.ai/ullm/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/se1aip5r' target=\"_blank\">https://wandb.ai/ullm/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset/runs/se1aip5r</a><br/> View project at: <a href='https://wandb.ai/ullm/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/ullm/Fine-tune%20Llama%203%208B%20on%20Medical%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240611_202348-se1aip5r/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"The model performance metrics are also stored under the specific project name on your Weights & Biases account.","metadata":{}},{"cell_type":"markdown","source":"Letâ€™s evaluate the model on a sample patient query to check if itâ€™s properly fine-tuned.\n\nTo generate a response, we need to convert messages into chat format, pass them through the tokenizer, input the result into the model, and then decode the generated token to display the text.\n","metadata":{}},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello doctor, I have bad acne. How do I get rid of it?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=150, \n                         num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:56:01.165028Z","iopub.execute_input":"2024-06-11T20:56:01.165286Z","iopub.status.idle":"2024-06-11T20:56:18.440305Z","shell.execute_reply.started":"2024-06-11T20:56:01.165262Z","shell.execute_reply":"2024-06-11T20:56:18.439333Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"\nHi. I would suggest you to use a combination of topical and oral medicines. For topical medicines, you can use a combination of benzoyl peroxide and salicylic acid. For oral medicines, you can use a combination of doxycycline and minocycline. You can also use a retinoid cream. You can also use a face wash containing tea tree oil. You can also use a face wash containing salicylic acid. You can also use a face wash containing glycolic acid. You can also use a face wash containing alpha hydroxy acid. You can also use a face wash\n","output_type":"stream"}]},{"cell_type":"code","source":"messages = [\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello doctor, I have fibromyalgia.What do you recommend?\"\n    }\n]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, \n                                       add_generation_prompt=True)\n\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, \n                   truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_length=150, \n                         num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:57:33.488120Z","iopub.execute_input":"2024-06-11T20:57:33.488779Z","iopub.status.idle":"2024-06-11T20:57:51.050177Z","shell.execute_reply.started":"2024-06-11T20:57:33.488744Z","shell.execute_reply":"2024-06-11T20:57:51.049228Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\nHi. I would recommend you to take a pain killer like Ibuprofen 400 mg three times a day. You can also take a muscle relaxant like Cyclobenzaprine 10 mg three times a day. You can also take a muscle relaxant like Tizanidine 2 mg three times a day. You can also take a pain killer like Tramadol 50 mg three times a day. You can also take a muscle relaxant like Baclofen 10 mg three times a day. You can also take a pain killer like Gabapentin 300 mg three times a day. You can also take\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**It turns out we can get average results even with one epoch.**","metadata":{}},{"cell_type":"markdown","source":"# Saving the model file\n\nWeâ€™ll now save the fine-tuned adapter and push it to the Hugging Face Hub. The Hub API will automatically create the repository and store the adapter file.","metadata":{}},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-11T20:58:01.567475Z","iopub.execute_input":"2024-06-11T20:58:01.568100Z","iopub.status.idle":"2024-06-11T20:58:10.681325Z","shell.execute_reply.started":"2024-06-11T20:58:01.568064Z","shell.execute_reply":"2024-06-11T20:58:10.680415Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /kaggle/input/llama-3/transformers/8b-chat-hf/1 - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bdc2140689b4c578a4e0443470f4b3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c76cc210e940b9bf75bbfc1ed1d1df"}},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/hordiales/llama-3-8b-chat-doctor/commit/a80c683414d406608b7aaad9686bd1206e250eda', commit_message='Upload model', commit_description='', oid='a80c683414d406608b7aaad9686bd1206e250eda', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"\nAs we can see, our save adapter file is significantly smaller than the base model.\n\nUltimately, weâ€™ll save the notebook with the adapter file to merge it with the base model in the new notebook.\n\nTo save the Kaggle Notebook, click the Save Version button at the top right, select the version type as Quick Save, open the advanced setting, select Always save output when creating a Quick Save, and then press the Save button.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}